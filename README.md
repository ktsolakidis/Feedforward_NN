# Feedforward_NN
Neural Network from Scratch: Sigmoid, Softmax, and Backpropagation

# Description

This project is my hands-on implementation of a neural network for multiclass classification. 
I built this network completely from scratch, using only Numpy and Matplotlib for visualizations. 
This is because while I am starting my journey into deep learning, i first want to make sure have a complete and in-depth understanding of how core machine learning and neural networks concepts like gradient descent/ascent, backpropagation and loos functions exactly work and update.

Iâ€™ve created a fully connected (dense) neural network with one hidden layer to classify synthetic 2D data into three distinct classes.
The focus is on implementing forward propagation, backpropagation, and parameter updates using gradient descent.

The data used in this project are artificial but it can - and soon will - be implemented with real world datasets.

Here we can see the class distribution of our generated data in a 2D space

<img width="989" alt="image" src="https://github.com/user-attachments/assets/32d084d3-8934-45ec-bc70-2e48e463affe">

and here the loss function minimization with the use of gradient ascent

<img width="952" alt="image" src="https://github.com/user-attachments/assets/d6f6cd17-28ea-4cea-b47b-67937dd76593">


Contact:

Konstantinos Tsolakidis  <br />
Machine Learning Engineer  <br />
Hatzakis Lab - University of Copenhagen  <br />
kontsolakidi25@gmail.com




